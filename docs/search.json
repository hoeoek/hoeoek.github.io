[
  {
    "objectID": "posts/rstudio-on-aws/index.html",
    "href": "posts/rstudio-on-aws/index.html",
    "title": "RStudio Server on AWS",
    "section": "",
    "text": "“Dark and ominous cloud” - DALLE2\n\n\nGoing digitally nomad is a dream for everyone, right? For the data scientist or ML operative, being able to get the work done no matter the location increase the resilience of any project. In addition, being able to leave that beefy workstation behind without sacrificing compute performance or tool availability is a game changer. Suddenly you’re tuning huge ensamble models using your phone in the back of a taxi, or doing EDA of a massive dataset using an ipad on an Italian roof terrace. Aside from personal convenience, I assume its better in terms of climate footprint to share resources with others in an ad hoc fashion instead of having everyone sitting around with tons of compute redundancies.\nSounds great - and in this post I’ll share some of my own experience with pursuing digital nomadness as a data scientist using cloud services - specifically by hosting an instance of RStudio Server on Amazon Web Services (AWS). Posit of course provides their own RStudio cloud, here I am just showing my current imperfect way of doing it - simply because I like to tinker around. This post assume you’re on a Linux system.\n\nOutline\n\nGet a virtual machine in the cloud using AWS EC2\nSet it up for Rstudio Server\nProfit???\n\n\n\nEC2 - elastic compute cloud\nAWS is kind enough to get beginners up to speed with their services by providing good tutorials and documentation, while also providing some free compute power. One of the great things about computing in the cloud is the flexibility. To get stuff up and running, you’re usually fine with the bare minimum of performance, then you can easily scale up the resources to make the virtual machine more powerful when needed.\n\n\nFirst forge the key…\nLets get this out of the way. In order to securely connect to your cloud resource, you need to generate SSH keypairs for authentication between the client (your computer) and the virtual machine instance (the aws server). There are of course several ways to do this, including being more or less secure about it, but the easiest way is to generate the key pair on your local machine ussing ssh-keygen:\n# Generate SSH keypair, accepting the defaults with enter\nssh-keygen \nCopy the public key:\n# Show your public key\ncat .ssh/<nameofkey>.pub\nGo to EC2 > Network and Security > Key Pairs > Import Key Pair, paste and save the public key.\n\n\n…Then build the fortress\nNow, find a suitable instance type for the VM. Find the “t3.micro and set it up for Ubuntu Server (I went for version 20). This is a weak virtual machine with only 1 GB of RAM and 2 vCPU’s. But its free tier, meaning that you can get a lot of use out of it for a limited time and when you need to actually start paying for it it costs about 1 cent per hour to use it. Compared to something like a raspberry pi, you can run the cloud machine for thousands of hours before hitting the pricetag of the pi. Make sure that you set the keypair you just created as a login keypair!\n\n\nElastic IP to the elastic compute cloud\nEvery time your instance is started or restarted, it gets assigned a new public IP address. This is a bit of a hurdle for your SSH’ing, as you need to log in to the AWS console, find the instance’s new Public IP, and update your connection settings for your SSH client in order to connect to your virtual machine. Luckily, there are a few different ways of dealing with this. You can get your own domain name from AWS (subscription based), hook up a cronjob with a no-ip solution on the VM, use AWS Route53, or some other clever thing. For a use case like this, I however prefer to settle for attaching the instance to an “elastic IP”. That is AWS’s own little DNS magic to route traffic between it’s static and dynamic IP’s. Its free if the instance is running, and cost a little bit when it’s not. See what they did there? Do you want to pay for the instance or for the elastic ip?\nAnyways, setup is super simple. just go to Network and Security > Elastic IP in your EC2 console, click Allocate Elastic IP address and attach it to your new instance. The allocated IPv4 adress will be associated to the instance, and you won’t have to worry about your SSH settings ever again.\n\n\nSSH into the machine, install software\n\n\n\n\n\n\nNote\n\n\n\nAny SSH client works for connecting to your EC2 VM, but I usually go with Termius since it has apps for the Apple ecosystem that sync connection settings across devices.\n\n\nSSH into your new virtual machine way up in Bezocloud using your new blingy auth keys and golden IP, then install R, RStudio server, and dependencies using these instructions (provided you’re on Ubuntu 20): R & RStudio Server\nNow we should be able to use any browser to connect to the RStudio web GUI on <your.elastic.ip>:8787.\nNext issue, since we’ve used SSH keypair authentication for the EC2 instance, we have no password to log in with. Instead of assigning our default user “ubuntu” with a password using passwd, we can make a separate user for our R business:\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that anyone may stumble across your RStudio web login form, so choose a strong user and password combination! Also take a second to ponder the user permissions so any evil redteamers can’t cause to much trouble on your cloud system.\n\n\n# add user \"rstudio-user\" and create a home dir for it\nsudo useradd -m rstudio-user \n\n# set a password rstudio-user\nsudo passwd rstudio-user\n\n# add a group in case you want to add more users later - just append to this group\nsudo groupadd rstudio-group\n\n# Put the user in the group\nsudo usermod -aG rstudio-group rstudio-user\nBasic security:\n# Disable shell access for rstudio-user\nsudo usermod -s /bin/false rstudio-user\nalso, open the RStudio config file with sudo nano /etc/rstudio/rserver.conf and add these lines:\n# Only authorize users in the rstudio-group to access rstudio\nauth-required-user-group=rstudio-group\n\n# Limit the number of concurrent sessions\nmax-sessions-per-user=2\nAnd restart the server\nsudo systemctl restart rstudio-server\nNow you should be able to log in to the RStudio web GUI using the new credentials.\nThere we have it! Our own, fresh RStudio environment in the cloud, accessible from any device with an internet connection. Remember that by default, it runs on the super non-secure HTTP protocol, so anyone on the same network could potentially listen in on all your traffic. I’ll go through how to fix that in a future post, as well as pairing the the compute environment with an S3 bucket for storage of data!"
  },
  {
    "objectID": "posts/prettifying_simple_graph/index.html",
    "href": "posts/prettifying_simple_graph/index.html",
    "title": "Prettifying a simple ggplot2 graph",
    "section": "",
    "text": "ggplot2 is an excellent tool for easily graphing data in all stages of the data analysis pipeline, whether for exploratory visualization, model performance, or prediction results. It manages to be both easy to use and powerful for tailoring plots to one’s preference - especially when considering add-on packages such as ggtext, GGAlly, wesanderson, patchwork, and more.\nin this post I show how I usually go about prettifying a simple graph. I find small adjustments to really enhance the eye-grabbyness of a plot, and could make all the difference at a conference poster session or presentation - as well as in a journal submission!\n >>> \nLibraries used:\n\nlibrary(tidyverse)\nlibrary(pxweb)\nlibrary(geomtextpath)\nlibrary(patchwork)\n\n\nGetting some data\nLet’s use pxweb to get some rando’ statistics from the Swedish Department of Energy’s publicly available API. pxweb_interactive() makes browsing PX API’s a breeze. This time I went for the table “Production of disintegrated unprocessed primary forest fuels of domestic origin by assortment, GWh”. Primary forest fuels are\n\n# Start the interactive inteface with a px endpoint\npxweb_interactive(\"https://pxexternal.energimyndigheten.se/api/v1/en\")\n\nThe console prints the commands needed to download the date you browse to, so it’s just a matter of copy/paste:\n\n# PXWEB query \npxweb_query_list <- \n  list(\"År\"=c(\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"),\n       \"Sortiment\"=c(\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"))\n\n# Download data \npx_data <- \n  pxweb_get(url = \"https://pxexternal.energimyndigheten.se/api/v1/en/Produktion,%20import%20och%20export%20av%20of%C3%B6r%C3%A4dlade%20tr%C3%A4dbr%C3%A4nslen/EN0122_3.px\",\n            query = pxweb_query_list)\n\nIt also spits out commands for turning the list into a dataframe, which we will clean a bit further by making it into a tibble and using the exquisite janitor package:\n\npff_init <-\n  as.data.frame(px_data,\n                column.name.type = \"text\",\n                variable.value.type = \"text\") %>%\n  tibble() %>%\n  janitor::clean_names() %>%\n  rename(production = starts_with(\"production\"))\n\n\n\nBasic plotting\n\npff_init %>% \n  ggplot(aes(year, production, group = assortment, color = assortment)) +\n  geom_line()\n\n\n\n\nOk, I admit the plot above is too basic to be presented in an article, on a presentation, or even a poster. But honestly, we all see this way too often.\nLet’s see how we can improve it, piece by piece. First, we’ll break it down a bit to enhance the interpretability by providing two perspectives that might help guide the reader towards the message potentially conveyed - ratios and totals.\nWe want to focus on the larger assortments in this dataset, it looks like some could be bunched together:\n\npff_init %>% \n  count(assortment, wt = production) %>% \n  rename(total_production = n)\n\n# A tibble: 6 × 2\n  assortment                      total_production\n  <chr>                                      <dbl>\n1 Branches and tops chips                    82719\n2 Fire wood                                  78346\n3 Residues from parks and gardens             2672\n4 Round wood chips                           50533\n5 Stumps chips                                1333\n6 Whole trees chips                           9973\n\n\nYup, let’s bunch ’em:\n\npff <- \n  pff_init %>% \n  mutate(assortment = if_else(\n     str_detect(assortment, \"Residues|Stumps|Whole\"),\n    \"Other\", assortment)) %>% \n  group_by(year, assortment) %>% \n  summarise(production = sum(production), .groups = \"keep\") %>% \n  arrange(production) %>% \n  ungroup()\n\nSo, lets start by creating a plot showing the relative change of PFF sources over time. To have control over label positions, i use mutate to iteratively plot and adjust the hjust and vjust arguments of geom_textpath.\n\n p1 <- \n  pff %>% \n  \n  # Prepare the data with ratios and convert to long format\n  pivot_wider(names_from = assortment, values_from = production) %>%\n  mutate(sums = rowSums(across(2:5))) %>%\n  mutate(across(2:5, ~ . / sums)) %>%\n  select(-6) %>%\n  pivot_longer(cols = 2:5) %>%\n  \n  # Mutate variables for more control of label positions and faceting\n  mutate(\n    h_just = case_when(\n      str_detect(name, \"Fire\") ~ 0.6,\n      str_detect(name, \"Branches\") ~ 0.75,\n      str_detect(name, \"Round\") ~ 0.7,\n      str_detect(name, \"Other\") ~ 0.7),\n    v_just = case_when(\n      str_detect(name, \"Fire\") ~ 2,\n      str_detect(name, \"Branches\") ~ -0.5,\n      str_detect(name, \"Round\") ~ 3.5,\n      str_detect(name, \"Other\") ~ 2),\n    name = if_else(str_detect(name, \"Branches\"), \"Branches\\nand tops chips\", name)\n    ) %>%\n  \n  # Start plotting!\n  ggplot(aes(year, value, group = name, color = name)) +\n  \n  # First, a basic line\n  geom_line(\n    linewidth = 1.2,\n    alpha = 0.8\n  )+\n  \n  # Then, add some points with nice white strokes\n  geom_point(aes(fill = name),\n             pch = 21,\n             size = 2,\n             stroke = 0.5,\n             color = \"white\",\n             alpha = 0.8)+\n  \n  # text along the plotted lines\n  geom_textpath(\n    aes(label = name, hjust = h_just, vjust = v_just),\n    linewidth = 1.1,\n    text_smoothing = 60, text_only = T) +\n  \n  # wesanderson adds some sweet color schemes based on iconic movies\n  scale_color_manual(\n    values = wesanderson::wes_palette(\n      \"Cavalcanti1\", type = \"continuous\", n = 4))+\n  scale_fill_manual(\n    values = wesanderson::wes_palette(\n      \"Cavalcanti1\", type = \"continuous\", n = 4))+\n  \n  # turn y values into percent\n  scale_y_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    # breaks = c(0, 0.05, 0.1, 0.3, 0.35, 0.4)\n    breaks = seq(0, 0.4, 0.05), limits = c(0, 0.4)\n  ) +\n  \n  \n  # set x axis labels\n  scale_x_discrete(breaks = as.character(c(2014, 2016, 2018, 2020, 2021))) +\n  \n  # annotation\n  labs(\n    title = \"Proportions of PFF production\",\n    x = NULL,\n    y = NULL\n  ) +\n  \n  # Theme magic\n  theme(\n    text = element_text(family = \"Helvetica\", size = 14),\n    plot.title = element_text(size = 12),\n    strip.text = element_blank(),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(colour = \"gray90\"),\n    panel.spacing = unit(10, \"mm\"),\n    axis.line.y = element_line(color = \"gray90\"),\n    legend.position = \"none\",\n    axis.ticks.length.y = unit(0, \"mm\")\n    ) \n\np1\n\n\n\n\nIt’s not perfect, but its a start! We should add a plot showing absolute values of the production - like a stacked area chart!\n\np2 <- \n  pff %>%\n  \n  # No need for labeling this plot - colors should be the same as before\n  ggplot() +\n  \n  # Stacked area chart\n  geom_area(\n    aes(year, production, group = assortment, fill = assortment),\n    color = \"white\", alpha = 0.9\n  )+\n  \n  # x axis labels, y axis reformatting, color assignment, and labels\n  scale_x_discrete(breaks = as.character(c(2014, 2016, 2018, 2020, 2021)))+\n  scale_y_continuous(labels = scales::number_format(scale = 0.001, suffix = \"k\"))+\n  scale_fill_manual(values = wesanderson::wes_palette(\"Cavalcanti1\", type = \"continuous\", n = 4))+\n  labs(\n    title = \"GWh produced from PFF\",\n    x = NULL,\n    y = NULL\n  ) +\n  \n  # Themin'! yay!\n  theme(\n    text = element_text(family = \"Helvetica\", size = 14),\n    plot.title = element_text(size = 12, family = \"sans\"),\n    strip.text = element_blank(),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(colour = \"gray90\"),\n    panel.spacing = unit(15, \"mm\"),\n    axis.line.y = element_line(color = \"gray90\"),\n    legend.position = \"none\",\n    axis.ticks.length.y = unit(0, \"mm\")\n  ) \n\nI love colors like this! Now to put it all together using patchwork:\n\np1 + p2 +\n  plot_annotation(title = \"Production of Primary Forest Fuels (PFF) in Sweden\",\n                  theme = theme(plot.title = element_text(family = \"serif\", size = 18)),\n                  caption = \"Source: Swedish Department of Energy\")\n\n\n\n\nIs it better? Maybe. I don’t pretend to propose that this is the way to go, this code is not super effective or anything. There’s still work to be done if this were to take center stage in a conference presentation or a poster - but for me the plot looks a lot more fun!"
  },
  {
    "objectID": "posts/first_post_beer_label/index.html",
    "href": "posts/first_post_beer_label/index.html",
    "title": "First Post: Beer Label",
    "section": "",
    "text": "In this very first post, I want to totally rush past the part of introducing myself and instead leave that to the About page. However, while the blog will be focused on data science, occasionally other things that I find interesting will sneak in. One of these things is beer, more specifically brewing beer - and what comes along with it. I have been brewing for a few years now and to make them feel a little more like the real deal, I like to design my own labels.\nIn this post I’ll share some R code for creating this map-style minimalist beer label that i recently used for a batch of IPA:\n\n\n\n\n\n\n\ntidyverse is the R metapackage to rule them all and includes a bunch of packages that are super useful for data wrangling. It also loads the wonderful magrittr pipe operator (%>%). osmdata is a wrapper for communicating with the Open Street Map API, and sf turns R into a geographic information system (GIS).\n\nlibrary(tidyverse) # The go-to library for basically doing anything in R nowadays\nlibrary(osmdata) # For downloading data from OpenStreetMap\nlibrary(sf) # For working with simple features, such as map stuff\n\n\n\n\nThis beer label will use the road layout for the city of Umeå in the north of Sweden. osmdata has some great functions for this.\n\ngetbb() returns the bounding box (coordinates of the 4 corners of a rectangle) associated with a geographical area.\nopq() builds the database query.\nadd_osm_features() adds, you’ve guessed it, our desired features to the query.\nosmdata_sf() Turns the results of the query into sf format.\n\nI’ll use highway features this time, but check Open Street Maps wiki for other features.\nI’ll separate the roads and streets into two sf features to be able to draw them using different line widths on the label. The output of osmdata_sf() will contain more than we need, so we extract just the line features using purrr::pluck()\n\n#choose area\nbbx <- getbb(\"Umeå, Sweden\")\n\n# large roads\nroads <- bbx %>%\n  opq() %>%\n  add_osm_feature(\n    key = \"highway\",\n    value = c(\"motorway\", \"trunk\", \"primary\",\n              \"secondary\", \"tertiary\", \"motorway_link\",\n              \"trunk_link\", \"primary_link\", \"secondary_link\",\n              \"tertiary_link\")\n    ) %>%\n  osmdata_sf() %>% \n  pluck(\"osm_lines\") \n\n# small roads\nstreets <- bbx %>%\n  opq() %>%\n  add_osm_feature(\n    key = \"highway\",\n    value = c(\"residential\", \"living_street\", \"service\",\n              \"unclassified\", \"pedestrian\", \"footway\",\n              \"track\", \"path\")\n    ) %>%\n  osmdata_sf() %>% \n  pluck(\"osm_lines\") \n\n\n\n\nIn this case I made the map-part of the label have a circular shape, with its center being somewhere downtown Umeå. I’ll get the coordinates non-programmatically (lazy) from just right clicking in Google Maps. Google Maps and OSM use the same coordinate system and projection, so the data smoothly slips into the processing pipeline without any conversion. Happy days.\nSince this is “just” a beer label, I won’t bother too much about dealing with projections - I’ll stick to WGS84/ESPG4326 which should produce good enough visual results.\n\n# make a point that'll be the circle center\ncenter <- c(lat = \"63.82400114332574\", long = \"20.262895955922296\")\n\n# Buffer the point to make the actual circle\ncircle <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%\n  st_buffer(dist = 3500, nQuadSegs = 200)\n\nNow we populate the circle with some roadage:\n\nroads_in_circle <- st_intersection(circle, roads)\nstreets_in_circle <- st_intersection(circle, streets)\n\nAnd plot our simple beer label using ggplot2:\n\nggplot() +\n  \n  # create the thinner streets\n  geom_sf(data = streets_in_circle,\n          linewidth = 0.3,\n          colour = \"gray30\") +\n  \n  # create the thicker streets\n  geom_sf(data = roads_in_circle,\n          linewidth = 0.5,\n          colour = \"gray10\") +\n  \n  # Set some labels\n  labs(title = \"UmeIPA\",\n       caption = \"Northern Made | Dry-Hopped\") +\n  theme_void() + # remove plot objects (x & y axis etc)\n  \n  # Theme our text to make it look fancier\n  theme(\n    plot.title = element_text(\n      family = \"mono\",\n      size = 30,\n      hjust = 0.5,\n      margin = margin(t = 10)\n    ),\n    plot.caption = element_text(\n      family = \"mono\",\n      size = 12,\n      hjust = 0.5,\n      margin = margin(b = 10)\n    )\n  )\n\n\n\n\nTo save it as an image, we may use:\n\nggsave(\"beer_label.png\", height = 6, width = 6, bg = \"white\")\n\nDoesn’t this make the hazy soup that stinks up your basement taste like what love feels like?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian Höök",
    "section": "",
    "text": "RStudio Server on AWS\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\nChristian\n\n\n\n\n\n\n  \n\n\n\n\nPrettifying a simple ggplot2 graph\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nChristian\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post: Beer Label\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nChristian\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "I am a PhD student working on leveraging machine learning to increase competitiveness for bio-based supply chains. With a strong background in forestry, I have honed my skills in statistical modeling in R and have contributed to research projects, providing data analysis and drone photogrammetry support. My teaching experience includes instructing master-level students in a forestry program, where I shared my passion for data-driven solutions towards enhancing supply chain resilience.\nI believe in the power of open-source technologies to democratize access to essential tools and data, allowing for more effective and inclusive decision-making in the bio-based industry and beyond."
  },
  {
    "objectID": "about.html#toolkit",
    "href": "about.html#toolkit",
    "title": "About this blog",
    "section": "Toolkit",
    "text": "Toolkit\n\nR (Tidyverse, Tidymodels)\nPython\nGIS\nGIT\nDocker\nQuarto\nAWS"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About this blog",
    "section": "Publications",
    "text": "Publications\nHöök (2022). Supply and Command: Quantitative Methods for Risk Management and Resilience in Wood Supply. Proceedings of The Joint 44th Annual Meeting of Council on Forest Engineering, the 54th International Symposium on Forest Mechanization. COFE;FORMEC. October 4-7, Corvallis, Oregon. ISBN: 979-8-9855282-1-3. https://www.formec.org/images/proceedings/2022/Proceedings_COFE-FORMEC-IUFRO_2022.pdf [2023-03-20]\nBergstrom et al. (2022). Effects of boom-corridor thinning on harvester productivity and residual stand structure. International Journal of Forest Engineering, 33. https://doi.org/10.1080/14942119.2022.2058258\nHöök (2020). Unmanned aerial vehicle photography and photogrammetry in the Smallwood project. (SmallWood report, November 2020). https://www.smallwood.eu/wp-content/uploads/sites/4/2021/01/SmallWood-Drone-Report_Christian_Hook.pdf [2023-02-18]\nHöök (2020). Promotional video of Smallwood project [Video]. https://www.youtube.com/watch?v=vkZ1UI9yZeo [2022-03-14]\nHöök et al. (2020). A Method of Finding HCT Roundwood Corridors for Reduction of GHG Emissions and Fuel Costs in Sweden. Forests, 11(2). https://doi.org/10.3390/f11020220"
  }
]